[{"id":0,"href":"/books/kustz/","title":"","parent":"","content":"Kustz 让应用在 Kubernetes 中管理更简单      kustz 的设计思想和定义     kustz 的一个核心理念就是 语义话， 换句话说就是具有 可读性 高， 见名知义。\n力求 kustz.yml 之于 应用， 就像 域名 至于 IP。\n对于一个服务应用来说， 所有的定义都在同一个配置文件里面， 不再割裂。\n从 kustz 的完整配置 中可以看到， 主要的参数都进行了 语义化 的处理和简化， 更贴近生活语言。\n## 1. k8s Deployment API 定义name:nginximage:docker.io/library/nginx:alpinereplicas:2envs:pairs:key1:value1configmaps:- srv-webapp-demo-envs:trueresources:cpu:10m/20mmemory:10Mi/20Minvidia.com/gpu:1/1probes:liveness:action:http://:8080/healthy## 2. k8s Service API 定义ports:- \u0026#34;80:8080\u0026#34;# cluster ip- \u0026#34;udp://!9998:8889\u0026#34;# 随机 nodeport# - \u0026#34;!20080:80:8080\u0026#34; # 指定 nodeport## 3. k8s Ingress API 定义ingress:rules:- http://api.example.com/ping?tls=star-example-com\u0026amp;svc=srv-webapp-demo:8080说明: ConfigMap 配置说明     由于默认的 kustomize 的生成器支持 k=v 格式， 不支持多行变量。 因此使用 liternals 实现。\n 注意， 变量文件值支持 YAML 格式文件。\n # kustz.yml# https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/secretgenerator/secrets:# 或 configmapsliterals:- name:srv-webapp-demo-literalsfiles:- foo.yml# type: Opaque # default变量文件\n## foo.yamlJAVA_HOME:/opt/java/jdkJAVA_TOOL_OPTIONS:-agentlib:hprofHTTPS_CERT:|---- RSA ---- asdflalsdjflasdjfl ---- RSA END ----既然现在的工具满足不了我们， 我们就自己抽象一层， 自己实现一个工具。\n为什么会有 kustz     你有没有想过， 如果要在 kubernetes 集群中 发布 一个最基本的 无状态服务， 并 提供 给用户访问， 最少需要配置几个 K8S Config API ?\n Deployment: 管理应用本身。 Service: 管理应用在集群内的访问地址， 也是应用在在集群累的负载均衡器。 Ingress: 管理应用对外暴露的入口， 通俗点说， 就是 URL。  前三个是最基本的的 API。\n如果还有配置文件或或者其他密钥管理， 可能你还需要。\nSecret / ConfigMap: 管理应用配置。  这些配置文件的存在， 本身都独立存在， 并没什么关系。\n为了让他们在一起， 你还需要定义 Label 信息， 并且通过 LabelSelector 将他们组合起来。\n只是将这些 Config API 文件组合在一起， 都是一件麻烦事情了。 这还不包括各个 Config API 本身的复杂结构， 以及不同版本之间的差别。\n社区也注意到这件事情了， 有很多工具帮我们组合管理， 例如我们今天要说的 Kustomize。\n除此之外， 还有微软和阿里云一起搞的 Open Application Model(简称 OAM)。\nKustomize     下面是 kustomize 最基本的配置文件 kustomization.yaml\n# kustomization.yamlapiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:demo-demoresources:- deployment.yml- service.yml- ingress.ymlconfigMapGenerator:- name:my-application-propertiesfiles:- application.properties更多参数， 可以到 kustomize 官网 查看。\n可以看到 kustomize 也只是帮我们完成了文件的组合， 并没有解决 Config API 复杂结构的问题。\n引用     ","description":"Kustz 让应用在 Kubernetes 中管理更简单      kustz 的设计思想和定义     kustz 的一个核心理念就是 语义话， 换句话说就是具有 可读性 高， 见名知义。\n力求 kustz.yml 之于 应用， 就像 域名 至于 IP。\n对于一个服务应用来说， 所有的定义都在同一个配置文件里面， 不再割裂。\n从 kustz 的完整配置 中可以看到， 主要的参数都进行了 语义化 的处理和简化， 更贴近生活语言。\n## 1. k8s Deployment API 定义name:nginximage:docker.io/library/nginx:alpinereplicas:2envs:pairs:key1:value1configmaps:- srv-webapp-demo-envs:trueresources:cpu:10m/20mmemory:10Mi/20Minvidia.com/gpu:1/1probes:liveness:action:http://:8080/healthy## 2. k8s Service API 定义ports:- \u0026#34;80:8080\u0026#34;# cluster ip- \u0026#34;udp://!9998:8889\u0026#34;# 随机 nodeport# - \u0026#34;!20080:80:8080\u0026#34; # 指定 nodeport## 3. k8s Ingress API 定义ingress:rules:- http://api.example.com/ping?tls=star-example-com\u0026amp;svc=srv-webapp-demo:8080说明: ConfigMap 配置说明     由于默认的 kustomize 的生成器支持 k=v 格式， 不支持多行变量。 因此使用 liternals 实现。"},{"id":1,"href":"/books/kustz/chapter01/01-introduce/","title":"01 Introduce","parent":"Chapter01","content":"介绍      如果要在 Kubernets 发布一个应用， 并对外提供服务， 需要配置诸如 Dep, Ing, Svc 等 Config API。 他们之间又是通过 Label 组合选择而实现的 松耦合。\n 如果想要这些 Config API 之间的关系更加紧密， 我们可以自己再向上抽象， 通过自己的配置将他们整合在一起。 更重要的是， 我们可以通过这层抽象， 屏蔽不同版本 API 之间差异。 实现同一个 kustz.yml 配置兼容多版本集群， 实现部署或迁移的丝滑。  Kustomize      kustomize: https://kubectl.docs.kubernetes.io/guides/introduction/kustomize/\n 现在这个官网的引导页面看起来已经有点乱了。\n简单的说， 以下是一个最基本的 kustomization.yaml 文件。\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:demo-demoresources:- deployment.yml- service.yml- ingress.yml ApiVersion 和 Kind : 对文件的作用定义 Namespace : 服务部署的运行环境。 Resources : 从外部引入的资源， 最终由 kustomize 统一渲染管理。 比如 patch 操作等。  Deployment, Pod 和 Container     先来说说 Deployment， 这个应该是最常见的 工作负载 workload， 定义 Pod 状态 。\n我们都知道， Pod 是 Kubernetes 中最小的 调度 单元， 定义网络、存储、权限等信息。\n换而言之， 最小的 执行 单元其实还是 Container， 定义了执行\n 通过 kubectl 命令，生成的最简单的 Deployment 模版。\n$ kubectl create deployment my-nginx --image nginx:alpine --dry-run=client -o yaml   最外层红色是 deployment. 中间层蓝色是 pod. 最内存绿色是 container.  没错， 他们之间的关系就是套娃， 一层套一层。\nkustz     kustz 的作用就如同 Deployment 一样， 将 应用 视作一个整体， 通过 某种 组织方式， 在一个文件中定义一个 应用/服务。\n 将所有的配置都集中到 同一个文件 中， 多个资源更方便管理。 将原本复杂的 API 结构 语义化， 配置起来更简单。  # kustz.ymlnamespace:demo-demo# 运行的命名空间service:# 定义一个应用name:srv-webapp-demoimage:docker.io/library/nginx:alpinereplicas:1envs:# 容器变量配置pairs:key1:value1-123resources:cpu:10m/20mmemory:10Mi/20Miprobes:# 容器探针liveness:action:http://:80/livenessports:# Service 端口- \u0026#34;80:80\u0026#34;# cluster ip## 对外暴露ingress:- http://www.example.com/* 注意: 以上配置文件结构，可能会随着代码开发进行调整。\n 如此这边，我们就可以通过 kustz.yml 定义完成一个服务的 完整配置定义 ， 之后再通过 kustz 工具将起转化为 kustomization.yml, deployment.yml ... 等文件， 最后通过 kubectl 工具进行发布管理。\n","description":"介绍      如果要在 Kubernets 发布一个应用， 并对外提供服务， 需要配置诸如 Dep, Ing, Svc 等 Config API。 他们之间又是通过 Label 组合选择而实现的 松耦合。\n 如果想要这些 Config API 之间的关系更加紧密， 我们可以自己再向上抽象， 通过自己的配置将他们整合在一起。 更重要的是， 我们可以通过这层抽象， 屏蔽不同版本 API 之间差异。 实现同一个 kustz.yml 配置兼容多版本集群， 实现部署或迁移的丝滑。  Kustomize      kustomize: https://kubectl.docs.kubernetes.io/guides/introduction/kustomize/\n 现在这个官网的引导页面看起来已经有点乱了。\n简单的说， 以下是一个最基本的 kustomization.yaml 文件。\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:demo-demoresources:- deployment.yml- service.yml- ingress.yml ApiVersion 和 Kind : 对文件的作用定义 Namespace : 服务部署的运行环境。 Resources : 从外部引入的资源， 最终由 kustomize 统一渲染管理。 比如 patch 操作等。  Deployment, Pod 和 Container     先来说说 Deployment， 这个应该是最常见的 工作负载 workload， 定义 Pod 状态 。"},{"id":2,"href":"/books/kustz/chapter02/01-sample-deployment/","title":"01 Sample Deployment","parent":"Chapter02","content":"2.1. 模仿 kubectl create 创建 Deployment 样例      为了简单， 我们假定所管理的 Deployment 都是 单容器 的。\n首先参考 kubectl create 命令\n$ kubectl create deployment my-dep --image=busybox --replicas 1 --dry-run=client -o yaml 安装 client-go API     访问 client-go https://github.com/kubernetes/client-go\n$ go get k8s.io/client-go@v0.25.4 这里直接选用最新版本 v0.25.4。 对于其他版本的兼容， 留在以后再做。\n定义 Kustz Config     参考 kubectl create 命令， 创建配置文件 kustz.yml 结构如下\n# kustz.ymlnamespace:demo-demoname:srv-webapp-demoservice:name:nginximage:docker.io/library/nginx:alpinereplicas:2在 service 中添加了 name 字段， 这是在 kubectl create 命令中没有的。 后者直接使用了镜像名称作为 name 的值。\n 由于我们的设计只有一个容器， 这里也可以 省略或使用默认值。 这里增加字段 完全是为了展示凑 API。\n 在 /pkg/kustz/kustz.go 中创建 Config， 对应所有字段。\ntype Config struct { Name string `json:\u0026#34;name\u0026#34;` Namespace string `json:\u0026#34;namespace\u0026#34;` Service Service `json:\u0026#34;service\u0026#34;` } type Service struct { Name string `json:\u0026#34;name\u0026#34;` Image string `json:\u0026#34;image\u0026#34;` Replicas int32 `json:\u0026#34;replicas\u0026#34;` } 拼凑 Deployment API     从这个标题就可以看出来， 这里就就没什么难度了， 就是把 kustz.Config 中的所有字段全部放在 Deployment API 中。\n为 kustz.Config 添加 KubeDeployment 方法， 在 /pkg/kustz/k_deployment.go 中。\nfunc (kz *Config) KubeDeployment() *appv1.Deployment { return \u0026amp;appv1.Deployment{ TypeMeta: metav1.TypeMeta{ Kind: \u0026#34;Deployment\u0026#34;, APIVersion: \u0026#34;apps/v1\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: kz.Name, Namespace: kz.Namespace, Labels: CommonLabels(*kz), }, Spec: appv1.DeploymentSpec{ Replicas: \u0026amp;kz.Service.Replicas, Template: kz.KubePod(), Selector: \u0026amp;metav1.LabelSelector{ MatchLabels: CommonLabels(*kz), }, }, } } 可以看到， 拼凑主要由 3 部分， 都是很熟悉的字段。\n TypeMeta: Deployment 的信息申明 ObjectMeta: 应用服务 的信息申明 Spec: 就是具体信息了。  如果你看的够仔细，可以在 Spec 中发现 Template 字段就开始 套娃 了。\nKubeDeployment 中调用了 /pkg/kustz/k_pod.go 中的 KubePod 方法。\nfunc (kz *Config) KubeDeployment() *appv1.Deployment { return \u0026amp;appv1.Deployment{ Spec: appv1.DeploymentSpec{ // ... 省略 \tTemplate: kz.KubePod(), }, } } KubePod 中调用了 /pkg/kustz/k_container.go 中的 KubeContainer 方法。\nfunc (kz *Config) KubePod() corev1.PodTemplateSpec { return corev1.PodTemplateSpec{ // ... 省略 \tSpec: corev1.PodSpec{ Containers: kz.KubeContainer(), }, } } 最后， 在 KubeContainer 方法中， 创建了最里面的 container 信息。\nfunc (kz *Config) KubeContainer() []corev1.Container { if kz.Service.Name == \u0026#34;\u0026#34; { kz.Service.Name = kz.Name } // ...省略 \treturn []corev1.Container{c} } 前面我们说到过 kz.Service.Name 为了展示 API 的拼凑。 为了以后使用方便， 这里我们设置其默认值为应用服务名 kz.Name 。\n公共标签     在 Kubernetes 中， 不同 API 之间的关系都是通过 标签选择 关联的。 为了方便， 在 /pkg/kustz/common.go 中使用函数 CommonLabels() 创建公共标签， 方便关联。\n文件解析     细心的你可以已经发现了， 明明配置文件用的是 yaml 格式， 但是在 Config 中的标签却是 json:\u0026quot;name\u0026quot;。\n这里只是为了 单纯 为了保障 yaml 库一致， 在 /pkg/kubeutils/yaml.go 中使用了 sigs.k8s.io/yaml 库而已, 这个库可以兼容 json, yaml 标签。\n测试     进入 kustz， 执行命令\n$ go test -v . $ kubectl create deployment srv-webapp-demo --image=nginx -n demo-demo --dry-run=client -o yaml 对比二者内容， 基本上完全一样了。\n","description":"2.1. 模仿 kubectl create 创建 Deployment 样例      为了简单， 我们假定所管理的 Deployment 都是 单容器 的。\n首先参考 kubectl create 命令\n$ kubectl create deployment my-dep --image=busybox --replicas 1 --dry-run=client -o yaml 安装 client-go API     访问 client-go https://github.com/kubernetes/client-go\n$ go get k8s.io/client-go@v0.25.4 这里直接选用最新版本 v0.25.4。 对于其他版本的兼容， 留在以后再做。\n定义 Kustz Config     参考 kubectl create 命令， 创建配置文件 kustz.yml 结构如下\n# kustz.ymlnamespace:demo-demoname:srv-webapp-demoservice:name:nginximage:docker.io/library/nginx:alpinereplicas:2在 service 中添加了 name 字段， 这是在 kubectl create 命令中没有的。 后者直接使用了镜像名称作为 name 的值。"},{"id":3,"href":"/books/kustz/chapter02/02-define-strings-to-service/","title":"02 Define Strings to Service","parent":"Chapter02","content":"2.2. 定义字符串创建 Service       大家好， 我是老麦， 一个小运维。 今天我们为 kustz 增加 service 解析功能。\n 通过 kubectl create service 命令可以看到， service 的模式还是挺多的。\n$ kubectl create service -h Create a service using a specified subcommand. Aliases: service, svc Available Commands: clusterip Create a ClusterIP service externalname Create an ExternalName service loadbalancer Create a LoadBalancer service nodeport Create a NodePort service 除了以上列出来的四种之外， 还用一种 Headless Service(https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)。\nHeadless Service 是当 类型 为 ClusterIP，且 IP 值为 None。 所以是 Cluster IP 的一种特殊情况。\n# 创建一个新的 ClusterIP service $ kubectl create service clusterip my-cs --tcp=5678:8080 # 创建一个新的 ClusterIP service (headless 模式) $ kubectl create service clusterip my-cs --clusterip=\u0026#34;None\u0026#34; Service API 中的 Port     如果你之前留意过 Service API， 你就应该会发现 API 中带有 port 的字段就有 3 个。 弄清楚他们分别对应什么这点很重要\n先来看看一个 NodePort 的 API。\n# $ kubectl create service nodeport my-cs --tcp=80:8080 --dry-run=client -o yamlapiVersion:v1kind:Service# ... 省略spec:ports:- name:80-8080nodePort:18080port:80protocol:TCPtargetPort:8080selector:app:my-cstype:NodePort对于 nodePort, port, targetPort 这三个名词如果还不能直接回答的话， 认真记住下面这张图。\n  集群外部用户通过 nodePort -\u0026gt; port -\u0026gt; targetPort。 集群内部用户通过 port -\u0026gt; targetPort。  kubez 中的 service 规则     在看随后的规则的时候， 再回头看看之前的 API， 能否找到几个关键要素。\n 端口映射规则。 Service 类型。 端口协议。 通常不太关注。 明确且有意义区分符号。 这是一个隐藏点。   clusterip: 80:8080。 无任何特殊符号。 headless: #80:8080。 注释符号 # 顾名思义， 就是把某些 Head 隐藏 之后成为 Headless。 nodeport: !18080:80:800 / !80:8080。 叹号 ! 表示重要， 因为我们要暴露端口到外部， 这是一个风险点。 NodePort 有两个规则， 是因为如果 不指定 端口就使用 随机 端口。 externalname: @example.com。 AT @， 指定或指向某个地方。 loadbalancer: %80:8080。 分号 %， 看起来就像个天平， 一人一半， 分流。  上面的规则中， 并没有提到 端口协议。\n 含有协议的规则 tcp://80:8080 / udp://!18080:80:8080。 就用协议地址的写法， 使用 :// 分割 协议 和 转发规则。  之所以协议拿出来单独说， 其原因是协议通常不太关注， 默认都用 TCP。因此在 kustsz 中也作为可省略字段。\n 以上规则都是个人习惯。 仅供参考。\n 代码实现     凑 API 还是很简单的， 不过需要注意当出现多个规则的时候怎样确定最终的 Type。\n例如规则中存在 ClusterIP, NodePort 和 ExternalName， 那在最终呈现的时候， Type 值是顺序第一？逆序第一？还是冲突报错？ 一切看自己需求和习惯。\nports:- \u0026#34;8099:80\u0026#34;- \u0026#34;!28080:80\u0026#34;- \u0026#34;@example.com\u0026#34;本章代码就先只实现两个， ClusterIP 和 NodePort。 完整配置查看 /pkg/kustz/kustz.yml\n# ... 省略service:# ... 省略ports:- \u0026#34;80:8080\u0026#34;# cluster ip- \u0026#34;udp://!9998:8889\u0026#34;# 随机 nodeport# - \u0026#34;!20080:80:8080\u0026#34; # 指定 nodeport先说一个变更     在 /pkg/kustz/common.go 中， 之前的 func CommonLabels(kz *Config) 函数变为了 func (kz *Config) CommonLabels() 方法。\nfunc (kz *Config) CommonLabels() map[string]string { return map[string]string{ \u0026#34;app\u0026#34;: kz.Name, } } 生成 Service API     在创建 Service API 的方法中还是中规中矩的在凑 API。\nfunc (kz *Config) KubeService() *corev1.Service { ports, typ := ParsePortStrings(kz.Service.Ports) svc := \u0026amp;corev1.Service{ TypeMeta: metav1.TypeMeta{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Service\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: kz.Name, Labels: kz.CommonLabels(), // Namespace: kz.Namespace, \t}, Spec: corev1.ServiceSpec{ Selector: kz.CommonLabels(), Type: typ, Ports: ports, }, } return svc } 在 ObjectMeta 中可以注意到， Namespace 字段是被注释了的。\n 因为 namespace 是环境限定， 而非服务本身特性。 即部署在 ns-a 和 ns-b 中， 服务配置本身并没有改变。 对于 namespace 值提供， 我想放在 kustomization.yml 中, 通过 kubectl apply -n ns-demo 提供。   同样的， 在 /pkg/kustz/k_deployment.go 中的 Deployment API 的 ObjectMeta 也删除了 namespace 字段。\n PortString     从 corev1.ServiceSpec 中可以看到， Type 和 Ports 是并列关系， 且只有一个 Type\nSpec: corev1.ServiceSpec{ Selector: kz.CommonLabels(), Type: typ, Ports: ports, }, 而在我们 kustz.yml 文件中, !9998:8889, Type 符号 存在于每一条规则中。\n因此， 通过函数 ParsePortStrings 解析所有规则， 并返回一个并列关系。\nfunc ParsePortStrings(values []string) ([]corev1.ServicePort, corev1.ServiceType) { // ... \tfor _, value := range values { port := NewPortFromString(value) if port.Type != corev1.ServiceTypeClusterIP { typ = port.Type } sps = append(sps, port.KubeServicePort()) } return sps, typ } 在 for 循环中， 处理了当 ports 出现多个 Type 规则的时候以谁为准的问题， 正如我们之前提到的。\n这里我使用了 Type 默认为 ClusterIP， 且最后出现的 非ClusterIP 为准（如有） 。\n为此， 我定义一个命令 PortString 的结构体，\n 通过 NewPortFromString 函数从 字符串 中提取关键信息。 并通过 PortString KubeServicePort 方法将结构体转换成的 corev1.ServicePort  type PortString struct { Port int32 TargetPort int32 NodePort int32 Protocol corev1.Protocol Type corev1.ServiceType } // NewPortFromString parse port from string to PortString func NewPortFromString(value string) PortString {} // KubeServicePort return a corev1.ServicePort func (p *PortString) KubeServicePort() corev1.ServicePort {} PortString 提供了相应的方法按规则从 string 提取关键信息。\n// toServiceClusterIP parse value from for ClusterIP func (p *PortString) toServiceClusterIP(value string) { parts := strings.Split(value, \u0026#34;:\u0026#34;) switch len(parts) { case 1: n := p.StringToInt32(parts[0]) p.Port = n p.TargetPort = n case 2: p.Port = p.StringToInt32(parts[0]) p.TargetPort = p.StringToInt32(parts[1]) } p.Type = corev1.ServiceTypeClusterIP } // toServiceNodePort parse value from for NodePort func (p *PortString) toServiceNodePort(value string) { value = strings.TrimPrefix(value, \u0026#34;!\u0026#34;) parts := strings.Split(value, \u0026#34;:\u0026#34;) switch len(parts) { case 1, 2: p.toServiceClusterIP(value) case 3: p.NodePort = p.StringToInt32(parts[0]) p.Port = p.StringToInt32(parts[1]) p.TargetPort = p.StringToInt32(parts[2]) } p.Type = corev1.ServiceTypeNodePort } 测试     在 /pkg/kustz/kustz_test.go 中， 拆分了 test 规则。\n执行命令， 检查结果是不是和自己期待的一样。\n$ go test -timeout 30s -run ^Test_KustzService$ ./pkg/kustz/ -v 如果不是， 就回去检查代码吧。\n","description":"2.2. 定义字符串创建 Service       大家好， 我是老麦， 一个小运维。 今天我们为 kustz 增加 service 解析功能。\n 通过 kubectl create service 命令可以看到， service 的模式还是挺多的。\n$ kubectl create service -h Create a service using a specified subcommand. Aliases: service, svc Available Commands: clusterip Create a ClusterIP service externalname Create an ExternalName service loadbalancer Create a LoadBalancer service nodeport Create a NodePort service 除了以上列出来的四种之外， 还用一种 Headless Service(https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)。\nHeadless Service 是当 类型 为 ClusterIP，且 IP 值为 None。 所以是 Cluster IP 的一种特殊情况。"},{"id":4,"href":"/books/kustz/chapter02/03-parse-url-to-ingress/","title":"03 Parse URL to Ingress","parent":"Chapter02","content":"2.3. 解析 URL 为 Ingress      大家好， 我是老麦， 一个运维小学生。 今天我们处理 Ingress， 对外提供服务。\n  之前已经提到过， 在 kustz.yml 中的字段值， 要尽量做到 见名知义。\n对于 Ingress 而言， 在发布之后， 我们访问的就是 URL 地址。\nhttp://api.example.com/v1 因此我们可以考虑 从结果推导解析渲染 Ingress 。\nKubernetes Ingress     老规矩， 我们还是通过命令看看创建一个 ingress 需要提供哪些参数。\n$ kubectl create ingress simple --rule=\u0026#34;foo.com/bar=svc1:8080,tls=my-cert\u0026#34; -o yaml --dry-run=client 在 rule 中， 提供了两组 k-v。 其中， foo.com/bar 就是一个不带协议的 URL。\n再来看看输出结果。\napiVersion:networking.k8s.io/v1kind:Ingressmetadata:creationTimestamp:nullname:simplespec:rules:- host:foo.com # 多 hosthttp:paths:- backend:# 一个 host 多个后端服务service:name:svc1port:number:8080path:/barpathType:Exacttls:- hosts:# 多个证书- foo.comsecretName:my-cert一个基本的 Ingress API 配置， 包含了\n 主要由两个模块 rules 和 tls 构成。 rules 是个数组， 即 一条或多条 URL。 每条规则可以有多个后端服务。 规则路径有一个特殊的 pathType 参数， 表示规则是 Prefix（前缀匹配） 还是 Excat（精确匹配）   补充， 还有一个重要的模块 annotations 通过声明控制 ingress-class 行为。\n $ kubectl create ingress annotated --class=default --rule=\u0026#34;foo.com/bar=svc:port\u0026#34; --annotation ingress.annotation1=foo 编码     有了之前 Service 打样， Ingress 就容易很多了。\nkustz.yml 配置     之前已经提到过了， 我们希望 ingress rule 所见即所得。\n# ...省略ingress:rules:- http://api.example.com/user/*?tls=my-cert\u0026amp;svc=srv-webapp-demo:8080- http://demo.example.com/login?tls=my-certannotations:k1:v1k2:v2 rule 就是最终实际对外暴露的 URL rule 通过 query 参数 tls 和 svc 传递后端证书名和服务。 为了区分 Prefix 和 Exact 两种 PathType， 我使用了 通配符 *。  Annotations     Annotation 还是很简单的， 本身就是 map 对象， 直接赋值就可以了。\ning := \u0026amp;netv1.Ingress{ TypeMeta: metav1.TypeMeta{ APIVersion: \u0026#34;networking.k8s.io/v1\u0026#34;, Kind: \u0026#34;Ingress\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: kz.Name, Labels: kz.CommonLabels(), Annotations: kz.Ingress.Annotations, }, // ... 省略 \t} Kubernetes 官方 IngressRule     在官方 IngressRule 结构体中规则描述还是挺多的。 截取部分\n 需要满足 RFC 3986 规范  host 字段不能是 IP 地址。 不支持 80,443 之外的端口。 即只支持 http 和 https 协议。   host 可以带 * 号， 即支持泛域名。   文档中还专门强调了， 这些规则在以后可能会改变。\n PathType     之前提到过， PathType 在控制匹配行为上非常重要。\n Prefix： 前缀匹配， 也可以理解为模糊匹配。 Exact: 精确匹配， 只有100%匹配才生效。 不建议使用 还有一个值 ImplementationSpecific， 表示由 ingress-class 决定值是 Prefix 或者 Excat。  PathType 其实虽说不难， 但是官网还是给了一个 Example 详细列举了匹配规则。 建议还是看一下。 https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types\n代码中\n Type 默认类型是 Excat。 相对比较安全。 通过判断 path 末尾最后一个字符是 * 则为 Prefix 规则。  func NewIngressRuleFromString(value string) *IngressRule { // ...省略 \t// ex: /api/* \tpath := ur.Path typ := netv1.PathTypeExact if strings.HasSuffix(path, \u0026#34;*\u0026#34;) { path = strings.TrimSuffix(path, \u0026#34;*\u0026#34;) typ = netv1.PathTypePrefix } // ...省略  确认 Prefix 之后， 别忘记把 * 从 path 中去掉。\n 渲染 Ingress     在官方 Ingress 结构体中字段一层套一层， 少说五六层。\n因此在代码中定义了一个 IngressRuleString 保存所需要的字段信息。\ntype IngressRuleString struct { Host string Path string PathType netv1.PathType TLSSecret string Service string }  通过函数 NewIngressRuleFromString 解析字符串。  func NewIngressRuleFromString(value string) *IngressRuleString { } 通过方法 KubeIngressTLS,KubeIngressRule 创建 tls 和 rule。  func (ir *IngressRuleString) KubeIngressTLS() *netv1.IngressTLS { } func (ir *IngressRuleString) KubeIngressRule() netv1.IngressRule { } IngressRuleString      注意， 在定义 IngressRuleString 的时候， 我偷了个懒。\n 前面说过， ingress rule 是支持多个后端服务的, 所以 Service 应该是 切片 类型。\ntype IngressRuleString struct { Service []string } 但在我的定义中是 字符串 类型。\n// 渲染 Ingress func (kz *Config) KubeIngress() *netv1.Ingress { rules, tlss := ParseIngreseRulesFromStrings(kz.Ingress.Rules, kz.Name) // ... 省略 } // 解析字符串 func ParseIngreseRulesFromStrings(values []string, defaultService string) ([]netv1.IngressRule, []netv1.IngressTLS) { // ... 省略 \tfor _, value := range values { ing := NewIngressRuleFromString(value) if ing.Service == \u0026#34;\u0026#34; { ing.Service = defaultService } } // ... 省略 }  在调用的时候传递了一个 服务同名 默认值。 服务同名 不包含端口， 即 srv-webapp-demo。 省略端口默认为 80  func (ir *IngressRuleString) toKubeIngressBackend() netv1.IngressBackend { // srv-webapp-demo[:8080] \tsvc := ir.Service port := int32(80) parts := strings.Split(svc, \u0026#34;:\u0026#34;) if len(parts) == 2 { svc = parts[0] port = StringToInt32(parts[1]) } } 这一处默认值 反过来要求了 Service 的端口映射必须是 80:xxxx\n# kustz.ymlservice:ports:- \u0026#34;80:8080\u0026#34;# cluster ip 为什么？\n 因为当管理多个服务（尤其是多个团队或语言的服务）时， 统一使用 80 为 Service 入口也可以作为一个强制规则约束， 节省脑细胞。\n  测试     执行命令， 检查结果是不是和自己期待的一样。\n$ go test -timeout 30s -run ^Test_KustzIngress$ ./pkg/kustz/ -v 如果不是， 就回去检查代码吧。\n","description":"2.3. 解析 URL 为 Ingress      大家好， 我是老麦， 一个运维小学生。 今天我们处理 Ingress， 对外提供服务。\n  之前已经提到过， 在 kustz.yml 中的字段值， 要尽量做到 见名知义。\n对于 Ingress 而言， 在发布之后， 我们访问的就是 URL 地址。\nhttp://api.example.com/v1 因此我们可以考虑 从结果推导解析渲染 Ingress 。\nKubernetes Ingress     老规矩， 我们还是通过命令看看创建一个 ingress 需要提供哪些参数。\n$ kubectl create ingress simple --rule=\u0026#34;foo.com/bar=svc1:8080,tls=my-cert\u0026#34; -o yaml --dry-run=client 在 rule 中， 提供了两组 k-v。 其中， foo.com/bar 就是一个不带协议的 URL。\n再来看看输出结果。\napiVersion:networking.k8s.io/v1kind:Ingressmetadata:creationTimestamp:nullname:simplespec:rules:- host:foo.com # 多 hosthttp:paths:- backend:# 一个 host 多个后端服务service:name:svc1port:number:8080path:/barpathType:Exacttls:- hosts:# 多个证书- foo."},{"id":5,"href":"/books/kustz/chapter02/04-kustomize/","title":"04 Kustomize","parent":"Chapter02","content":"2.4. kustomize 流水线       大家好， 我是老麦， 一个运维小学生。\n 前面已经简单的封装了 Deployment, Service, Ingress， 完成了零部件的创建。\n今天就通过 Kustomization 进行组装， 实现流水线。\nKustomize     开始之前， 先来安装 kustomize 库。\n$ go get sigs.k8s.io/kustomize/v3 这里补充一下， 访问 Github https://github.com/kubernetes-sigs/kustomize/。\nkustomize () 首页 README.md 并没有提到 go get 的包名。 通常 k8s 的代码在 github 上都是镜像。 这时候只需要进到 go.mod ， 包名就一目了然。\n// go.mod module sigs.k8s.io/kustomize/v3 go 1.12 编码     先来看看 kustomization.yml 的定义， 非常的简单。\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:demo-demoresources:- deployment.yml- service.yml- ingress.yml今天的代码及其简单， 只需要 20 行搞定。 在 import 的时候， 可能自动补全不会自己带上 v3。 需要手工调整一下。\npackage kustz import \u0026#34;sigs.k8s.io/kustomize/v3/pkg/types\u0026#34; func (kz *Config) Kustomization() types.Kustomization { k := types.Kustomization{ TypeMeta: types.TypeMeta{ Kind: types.KustomizationKind, APIVersion: types.KustomizationVersion, }, Namespace: kz.Namespace, Resources: []string{ \u0026#34;deployment.yml\u0026#34;, \u0026#34;service.yml\u0026#34;, \u0026#34;ingress.yml\u0026#34;, }, } return k } 这里已经定了 kustomization 三个外部资源名字。\n其它     kustomize 还是很贴心的， 在 types 把 version 和 kind 已经通过常量定义好了。\n在 https://github.com/kubernetes-sigs/kustomize/blob/v3.3.1/pkg/types/kustomization.go\nconst ( KustomizationVersion = \u0026#34;kustomize.config.k8s.io/v1beta1\u0026#34; KustomizationKind = \u0026#34;Kustomization\u0026#34; ) 另外我们可以看到， 虽然 TypeMeta 定义相同， 但是直接从  apimachinery/pkg/apis/meta/v1.TypeMeta 复制过来的， 而不是通过引用。\n// TypeMeta partially copies apimachinery/pkg/apis/meta/v1.TypeMeta // No need for a direct dependence; the fields are stable. type TypeMeta struct { Kind string `json:\u0026#34;kind,omitempty\u0026#34; yaml:\u0026#34;kind,omitempty\u0026#34;` APIVersion string `json:\u0026#34;apiVersion,omitempty\u0026#34; yaml:\u0026#34;apiVersion,omitempty\u0026#34;` }  之前看到一句话， 简单的拷贝比引用可能更节约资源， 因为引用是初始化一整个包\n 测试     执行命令， 检查结果是不是和自己期待的一样。\n$ go test -timeout 30s -run ^Test_KustzKustomize$ ./pkg/kustz/ -v 如果不是， 就回去检查代码吧。\n","description":"2.4. kustomize 流水线       大家好， 我是老麦， 一个运维小学生。\n 前面已经简单的封装了 Deployment, Service, Ingress， 完成了零部件的创建。\n今天就通过 Kustomization 进行组装， 实现流水线。\nKustomize     开始之前， 先来安装 kustomize 库。\n$ go get sigs.k8s.io/kustomize/v3 这里补充一下， 访问 Github https://github.com/kubernetes-sigs/kustomize/。\nkustomize () 首页 README.md 并没有提到 go get 的包名。 通常 k8s 的代码在 github 上都是镜像。 这时候只需要进到 go.mod ， 包名就一目了然。\n// go.mod module sigs.k8s.io/kustomize/v3 go 1.12 编码     先来看看 kustomization.yml 的定义， 非常的简单。"},{"id":6,"href":"/books/kustz/chapter02/05-kustz-cli/","title":"05 Kustz CLI","parent":"Chapter02","content":"2.5. 使用 cobra 实现 kustz 命令      大家好， 我是老麦。 一个运维小学生。\n 有了前面几章的努力， 我们的命令行工具 kustz 终于要问世了。\nkustz 命令     当前命令功能就很简单。\n default: 输出 kustz 默认配置。 render: 读取 kustz 配置并生成 kustomize 配置四件套。  $ kustz -h Available Commands: default 在屏幕上打印 kustz 默认配置 render 读取 kustz 配置， 生成 kustomize 所需文件 编码     本章的代码都很简单， 就是设计的文件比较多。\n使用 cobra 创建命令     cobra 真的是一个非常好用的命令行工具。\ngo get -u github.com/spf13/cobra 在 /cmd/kustz/cmd/root.go 中创建 根命令 rootCmd。 并定义 执行函数 Execute。\nfunc Execute() error { return rootCmd.Execute() } 在 /cmd/kustz/cmd/default.go 中创建 子命令 default， 无任何参数。 在 /cmd/kustz/cmd/render.go 中创建 子命令 render， 有一个参数 config， 实现根据配置管理应用。\n而在外部 /cmd/kustz/main.go 中， 只有一个入口函数 main 调用 rootCmd 的执行。 保持文件清洁干爽。\n    花开两朵。 在 /pkg/kustz/cmd.go 文件中， 提供了 函数 或 方法 供之前的命令调用\ndefault     使用 go:embed 将配置文件 kustz.yml 嵌入到应用中。 配置随着代码走， 测试分发两不误。\n//go:embed kustz.yml var defaultConfig string func DefaultConfig() { fmt.Println(defaultConfig) } 这里只是简单的配置文件打印标准输出。 如果需要保存到文件， 用户可以自行使用 重定向符。\nrender     通过 RenderAll 方法将之间的 Deployment, Ingress, Service, Kustomization 都保存成了对应文件。\nfunc (kz *Config) RenderAll() error { } 在 Kustomize 章节已经硬编码 Resources 的资源文件名称。 因此这里可就定义了这几个文件名的常量。\nconst ( FileDeployment = \u0026#34;deployment.yml\u0026#34; FileIngress = \u0026#34;ingress.yml\u0026#34; FileService = \u0026#34;service.yml\u0026#34; FileKustomization = \u0026#34;kustomization.yml\u0026#34; ) 在 /pkg/kubeutils/yaml.go 中， 将 dep, svc, ing 等编码成 YAML 的时候， 用到了 sigs.k8s.io/yaml (k8syaml) 库， 而非 gopkg.in/yaml.v2 (pkgyaml)。 跟踪一下 k8syaml 的代码就很容易知道， 前者在 pkgyaml 的基础上， 针对性的为 k8s 做了很多优化。\n编译及测试     $ go build ./cmd/kustz $ ./kustz default \u0026gt; abc.yml $ ./kustz render -c abc.yml 如果不行， 检查一下代码吧。\n","description":"2.5. 使用 cobra 实现 kustz 命令      大家好， 我是老麦。 一个运维小学生。\n 有了前面几章的努力， 我们的命令行工具 kustz 终于要问世了。\nkustz 命令     当前命令功能就很简单。\n default: 输出 kustz 默认配置。 render: 读取 kustz 配置并生成 kustomize 配置四件套。  $ kustz -h Available Commands: default 在屏幕上打印 kustz 默认配置 render 读取 kustz 配置， 生成 kustomize 所需文件 编码     本章的代码都很简单， 就是设计的文件比较多。\n使用 cobra 创建命令     cobra 真的是一个非常好用的命令行工具。"},{"id":7,"href":"/books/kustz/chapter03/01-container-env-var/","title":"01 Container Env Var","parent":"Chapter03","content":"3.1. 为 Container 添加环境变量      大家好， 我是老麦。 一个运维小学生。 今天为容器添加环境变量。\n  再前面一章中， 我们已经完成了 Deployment, Service, Ingress 和 Kustomization API 的封装。 并通过 cobra 库创建了属于我们自己的 kustz 命令。\n然而 kustz 的功能还简陋。 今天我们就先来为容器添加环境变量。\n为容器设置环境变量     在官方文档中， 提高了两种为容器设置环境变量的方法\n https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/\n  env: 提供 k-v 模式 键值对。  值可以直接 value 提供。 也可以通过 valueFrom 从 secret 或 configmap 引用。   envFrom: 从 secret 或 configmap 中读取键值对， 注入到容器中。。  https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/    kubez.yml 配置     首先来看看 kubez.yml 的配置\n# kubez.ymlservice:envs:pairs:key1:value1files:- foo.yml- bar.yml我设计了两种方式为容器提供环境变量。 都是提供 k-v 模式。\n# deployment.ymlenv:- name:DEMO_GREETINGvalue:\u0026#34;Hello from the environment\u0026#34; pairs: k-v 模式。 优先级更高， 可以覆盖 files 中出现的同名 k-v。 files: 从文件中读取 k-v。  多个 kustz.yml 可以复用。 可以按类型分类， 更直观。 例如工程变量和数据库变量。 选择 YAML 格式是为了更好的管理 值为多行的变量。 比如证书。 同名变量，后者覆盖前者     挖个坑， 以后实现 2.2 中提到的数据库变量文件的加解密。 让 GitOPS 更安全一点。\n 最后强调一下变量优先级顺序， 用链条表示: 后者覆盖前者。\nfoo.yml \u0026lt;- bar.yml \u0026lt;- pairs 编码实现     在 /pkg/kustz/kustz.go 中， 增加配置字段。 这个很简单， 就不赘述了。\ntype ServiceEnvs struct { Pairs map[string]string `json:\u0026#34;pairs,omitempty\u0026#34;` Files []string `json:\u0026#34;files,omitempty\u0026#34;` } 在引用 ServiceEnvs 的时候没有始终指针。 这样 Service 在初始化的时候， ServiceEnvs 即使在 kustz.yml 没有定义也会被初始化为 空 的零值。\ntype Service struct { Envs ServiceEnvs `json:\u0026#34;envs,omitempty\u0026#34;` } 代码还是很简单的， 在 /pkg/kustz/k_container.go 定义了方法 kubeContainerEnv。 该方法中， 指定了变量的优先级。\nfunc (kz *Config) kubeContainerEnv() []corev1.EnvVar { pairs := make(map[string]string, 0) for _, file := range kz.Service.Envs.Files { b, _ := os.ReadFile(file) _ = yaml.Unmarshal(b, \u0026amp;pairs) } for k, v := range kz.Service.Envs.Pairs { pairs[k] = v } return tokube.ContainerEnv(pairs) } 新建了一个包 tokube， 这里面的函数通过 接受 参数返回 API 配置。\n在 /pkg/tokube/container_env.go 定义了函数 ContainerEnvs 创建容器变量键值对。\nfunc ContainerEnv(pairs map[string]string) []corev1.EnvVar { envs := []corev1.EnvVar{} for k, v := range pairs { envs = append(envs, corev1.EnvVar{ Name: k, Value: v, }) } return envs } 说一个容易错的点     在 Container API 中， 变量保存在 []corev1.EnvVar， 这是一个切片。 切片的另一个 隐藏含义 就是可能出现 同名 KEY。\n第一次的代码如下，\nfunc (kz *Config) kubeContainerEnv_Error() []corev1.EnvVar { envs := []corev1.EnvVar{} // 注意这里所有变量全部假如了 envs 切片。 \tenvs = append(envs, tokube.ContainerEnv(kz.Service.Envs.Pairs)...) for _, file := range kz.Service.Envs.Files { b, _ := os.ReadFile(file) mm := make(map[string]string, 0) _ = yaml.Unmarshal(b, \u0026amp;mm) envs = append(envs, tokube.ContainerEnv(mm)...) } return envs }  并没有测试这样的变量结构会出现什么情况， 因为这种情况就不应该出现。\n测试     觉得之前的测试命令不方便， 更新了 Makefile， 添加了测试命令。 执行命令测试吧。\n$ make test.deployment ","description":"3.1. 为 Container 添加环境变量      大家好， 我是老麦。 一个运维小学生。 今天为容器添加环境变量。\n  再前面一章中， 我们已经完成了 Deployment, Service, Ingress 和 Kustomization API 的封装。 并通过 cobra 库创建了属于我们自己的 kustz 命令。\n然而 kustz 的功能还简陋。 今天我们就先来为容器添加环境变量。\n为容器设置环境变量     在官方文档中， 提高了两种为容器设置环境变量的方法\n https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/\n  env: 提供 k-v 模式 键值对。  值可以直接 value 提供。 也可以通过 valueFrom 从 secret 或 configmap 引用。   envFrom: 从 secret 或 configmap 中读取键值对， 注入到容器中。。  https://kubernetes."},{"id":8,"href":"/books/kustz/chapter03/02-configmap-secret-generator/","title":"02 Configmap Secret Generator","parent":"Chapter03","content":"3.2. [kustz] ConfigMap 和 Secret 的生成器      大家好， 我是老麦， 一个运维小学生。 今天我们通过 kustomize 管理 ConfigMap 和 Secret。\n  上一节我们通过 k-v 和 YAML文件 为容器添加环境变量。 同时也提到了可以通过 envFrom 这个关键字， 直接读取 ConfigMap 或 Secret 中的 k-v 作为容器的环境变量。\n除了环境变量之外， ConfigMap 和 Secret 还能管理的东西还很多。 所以我个人觉得单应用管理部署的话， 对于配置的管理，还是比较重要的。\nKustomize 中的 ConfigMap Env File     在 kustzomize 中， ConfigMap 和 Secret 都是通过 生成器 Generator 管理的， 有很多配置。\n https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/generatoroptions/\n 先切到 ConfigMapGenerator， 可以看到有三种模式提供数据, files , literals, envs。\n如果按照我们之前说的， 为容器提供环境变量， 使用 envs 是最方便的。 从名字就可以看到， 就是为了环境变量而提供的。\n https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/configmapgenerator/#configmap-from-env-file\n 但是这种模式提供数据有也有限制的\n 必须使用 key=value 这种结构  但这并不是 SHELL 变量赋值的形式   每一对 k-v 只能是单行。 key 作为变量名还好说， value 作为值就 不能支持多行 数据了。  另外 value 中的所有字符都是字面值。    举个例子\nHTTPS_CERT=\u0026#34;---RSA---\\nasdfjal\\n---END\u0026#34; 通常在 Shell 中 \u0026quot; 双引号 是可以保留 \\n 换行符 转义的含义的。 但是在这里 \u0026quot; 和 \\n 都是字面意思， 没有任何特殊。\nConfigMap / Secret 生成器     看看定义， ConfigMapArgs 和 SecretArgs\n 都是通过 GeneratorArgs 管理数据的。 Secret 比 ConfigMap 多了一个 Type 字段。  type ConfigMapArgs struct { GeneratorArgs `json:\u0026#34;,inline,omitempty\u0026#34; yaml:\u0026#34;,inline,omitempty\u0026#34;` } // SecretArgs contains the metadata of how to generate a secret. type SecretArgs struct { GeneratorArgs `json:\u0026#34;,inline,omitempty\u0026#34; yaml:\u0026#34;,inline,omitempty\u0026#34;` // This is the same field as the secret type field in v1/Secret: \t// It can be \u0026#34;Opaque\u0026#34; (default), or \u0026#34;kubernetes.io/tls\u0026#34;. \tType string `json:\u0026#34;type,omitempty\u0026#34; yaml:\u0026#34;type,omitempty\u0026#34;` } 对于数据源我计划都从文件中读取。 这样三个模式就有了相同过的抽象结构体。 抽象结果为 Generator 结构体， 在 /pkg/kustz/kustz.go 中可以看到。\nkustz.yml 配置      在 kustz.yml 中新增加了两个字段 configmaps, secrets。 每个字段都有三个模式 envs, files, literals。 每个模式都有三个字段  name: 最终生成的 ConfigMap 或 Secret 名字。 files: 数据源。 [target_name=]source_name。 target_name 就是 ConfigMap 中的文件 key。 如省略， 默认与 source_name 相同。 type: 类型。 Secret 专有。 取值范围参考 https://kubernetes.io/docs/concepts/configuration/secret/#secret-types     # kustz.yml# https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/configmapgenerator/configmaps:envs:- name:srv-webapp-demo-envsfiles:- src_name.txt# https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/secretgenerator/secrets:literals:- name:srv-webapp-demo-literalsfiles:- foo.yml# type: Opaque # defaultfiles:- name:srv-webapp-demo-filesfiles:- tls.crt=catsecret/tls.crt- tls.key=secret/tls.keytype:\u0026#34;kubernetes.io/tls\u0026#34;编码     type Config struct { ConfigMaps Generator `json:\u0026#34;configmaps\u0026#34;` Secrets Generator `json:\u0026#34;secrets\u0026#34;` } // Generator 定义数据源种类 type Generator struct { Literals []GeneratorArgs `json:\u0026#34;literals,omitempty\u0026#34;` Envs []GeneratorArgs `json:\u0026#34;envs,omitempty\u0026#34;` Files []GeneratorArgs `json:\u0026#34;files,omitempty\u0026#34;` } // GeneratorArgs 定义数据源类型参数 type GeneratorArgs struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` Files []string `json:\u0026#34;files,omitempty\u0026#34;` Type string `json:\u0026#34;type,omitempty\u0026#34;` } Generator 也就承担关于 ConfigMap 和 Secret 所有工作。\n在 /pkg/kustz/k_kustomize.go 中， 为 Generator 创建了两个方法创建对应参数。\n// toConfigMapArgs 返回 ConfigMap 参数 func (genor *Generator) toConfigMapArgs() []types.ConfigMapArgs { args := []types.ConfigMapArgs{} for _, data := range genor.datas() { for _, garg := range data.gargs { arg := tokust.ConfigMapArgs(garg.Name, garg.Files, data.mode) args = append(args, arg) } } return args } // toSecretArgs 返回 Secret 参数 func (genor *Generator) toSecretArgs() []types.SecretArgs {} 由于 ConfigMap 和 Secret 确实太过相似， 所以对于处理 GeneratorArgs 使用循环， 从而添加了一个 Mode 类型的概念。 这个 Mode 的取值范围就是 envs, literals, files。\ntype GeneratorArgsData struct { mode tokust.GeneratorMode gargs []GeneratorArgs } // datas 整合生成器数据 func (genor *Generator) datas() []GeneratorArgsData { return []GeneratorArgsData{ {mode: tokust.GeneratorMode_Envs, gargs: genor.Envs}, {mode: tokust.GeneratorMode_Files, gargs: genor.Files}, {mode: tokust.GeneratorMode_Literals, gargs: genor.Literals}, } } 在 /pkg/tokust/generator.go 文件中， 定义了几个函数创建 kustomize 对象的方法。\nfunc ConfigMapArgs(name string, files []string, mode GeneratorMode) types.ConfigMapArgs { } func SecretArgs(name string, files []string, typ string, mode GeneratorMode) types.SecretArgs { // 处理默认类型 \tif typ == \u0026#34;\u0026#34; { typ = \u0026#34;Opaque\u0026#34; } } 相应的， 也创建三种模式的对应的方法。\nfunc generatorArgs_literals(name string, files []string) types.GeneratorArgs { data := make(map[string]string, 0) for _, file := range files { err := marshalYaml(file, data) if err != nil { panic(err) } } sources := mapToSlice(data) // ... 省略 } func generatorArgs_files(name string, files []string) types.GeneratorArgs { } func generatorArgs_envs(name string, files []string) types.GeneratorArgs { } 在 literals 中， 由于我们传入的是 文件， 但是在 kustomization.yml 是键值对。 所以多了一个读取数据的步骤， 并且定义了一个规则， 如果出现同名变量， 后面出现的覆盖先出现的。\n测试     执行命令， 查看结果。\n$ make test.kustomize  这里不会直接生成 ConfigMap 和 Secret， 而是生成 Kustomization.yml 规则。\n ","description":"3.2. [kustz] ConfigMap 和 Secret 的生成器      大家好， 我是老麦， 一个运维小学生。 今天我们通过 kustomize 管理 ConfigMap 和 Secret。\n  上一节我们通过 k-v 和 YAML文件 为容器添加环境变量。 同时也提到了可以通过 envFrom 这个关键字， 直接读取 ConfigMap 或 Secret 中的 k-v 作为容器的环境变量。\n除了环境变量之外， ConfigMap 和 Secret 还能管理的东西还很多。 所以我个人觉得单应用管理部署的话， 对于配置的管理，还是比较重要的。\nKustomize 中的 ConfigMap Env File     在 kustzomize 中， ConfigMap 和 Secret 都是通过 生成器 Generator 管理的， 有很多配置。\n https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/generatoroptions/\n 先切到 ConfigMapGenerator， 可以看到有三种模式提供数据, files , literals, envs。"},{"id":9,"href":"/books/kustz/chapter03/03-container-env-from/","title":"03 Container Env From","parent":"Chapter03","content":"3.3. [kustz] 注入 ConfigMap 和 Secrets 到容器环境变量       大家好， 我是老麦。 一个运维小学生。\n 有了前面两张的铺垫， 今天这个很简单。 我们说说另外一种为容器注入环境变量的方式。\n容器变量注入 EnvFrom     前面我们提到过， Container 有两种方式定义环境变量， 其中一种就是 envFrom， 从 ConfigMap 或 Secret 中读取所有键值对作为容器的变量。\nConfigMap 和 Secret 看起来是这样的。 数据都在 data 字段。\napiVersion:v1data:APP_NAME:gin-demoLOG_LEVEL:debugkind:ConfigMapmetadata:name:config-demo在定义引用的时候， 使用 envFrom 关键字， 参考官网案例 https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/\napiVersion:v1kind:Podmetadata:name:dapi-test-podspec:containers:- name:test-containerimage:registry.k8s.io/busyboxcommand:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;env\u0026#34;]envFrom:- configMapRef:name:special-config# optional: falserestartPolicy:Never官网的 demo 中并没有提及 optional 这个字段， 但是在后面 限制条件 的时候做了详细解释。\n 限制\n  在 Pod 规约中引用某个 ConfigMap 之前，必须先创建这个对象， 或者在 Pod 规约中将 ConfigMap 标记为 optional（请参阅可选的 ConfigMaps）。 如果所引用的 ConfigMap 不存在，并且没有将应用标记为 optional 则 Pod 将无法启动。 同样，引用 ConfigMap 中不存在的主键也会令 Pod 无法启动，除非你将 Configmap 标记为 optional。 如果你使用 envFrom 来基于 ConfigMap 定义环境变量，那么无效的键将被忽略。 Pod 可以被启动，但无效名称将被记录在事件日志中（InvalidVariableNames）。 日志消息列出了每个被跳过的键。例如:   可选的 ConfigMap 你可以在 Pod 规约中将对 ConfigMap 的引用标记为 可选（optional）。 如果 ConfigMap 不存在，那么它在 Pod 中为其提供数据的配置（例如环境变量、挂载的卷）将为空。 如果 ConfigMap 存在，但引用的键不存在，那么数据也是空的。\n  optional 默认值为 false , 即配置文件必须存在，否则会报错。\n 编码     今天的编码很简单， 就几句话。\nkustz.yml 配置     在 service.envs 中增加两个字段 configmaps 和 secrets。 他们都是 字符串切片。\n# kustz.ymlservice:name:nginximage:docker.io/library/nginx:alpineenvs:configmaps:# - name:optional- srv-webapp-demo-envs:truesecrets:- srv-webapp-demo-envs # default optional:false字符串分两段 name:optional\n解析字符串为 API 对象     代码很简单， 没什么好说的。\n 在 /pkg/tokube/container_env.go 中， 函数 ParseEnvFromSource 解析字符串为 corev1.EnvFromSource 对象。 在 /pkg/kustz/k_container_env.go 中， 循环遍历 configmaps 和 secrets 获取字符串。   补充说明\n 在 corev1.EnvFromSource 中， 有个字段叫 Prefix 是用于给变量 加前缀 的。\n// EnvFromSource represents the source of a set of ConfigMaps type EnvFromSource struct { // An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. \t// +optional \tPrefix string `json:\u0026#34;prefix,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=prefix\u0026#34;` } 我更喜欢 所见即所得， 所以我并没有处理这个字段。 另外环境变量是服务定义的， 也用不着我们画蛇添足。\n测试     执行命令查看结果\n$ make test.deployment ","description":"3.3. [kustz] 注入 ConfigMap 和 Secrets 到容器环境变量       大家好， 我是老麦。 一个运维小学生。\n 有了前面两张的铺垫， 今天这个很简单。 我们说说另外一种为容器注入环境变量的方式。\n容器变量注入 EnvFrom     前面我们提到过， Container 有两种方式定义环境变量， 其中一种就是 envFrom， 从 ConfigMap 或 Secret 中读取所有键值对作为容器的变量。\nConfigMap 和 Secret 看起来是这样的。 数据都在 data 字段。\napiVersion:v1data:APP_NAME:gin-demoLOG_LEVEL:debugkind:ConfigMapmetadata:name:config-demo在定义引用的时候， 使用 envFrom 关键字， 参考官网案例 https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/\napiVersion:v1kind:Podmetadata:name:dapi-test-podspec:containers:- name:test-containerimage:registry.k8s.io/busyboxcommand:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;env\u0026#34;]envFrom:- configMapRef:name:special-config# optional: falserestartPolicy:Never官网的 demo 中并没有提及 optional 这个字段， 但是在后面 限制条件 的时候做了详细解释。\n 限制\n  在 Pod 规约中引用某个 ConfigMap 之前，必须先创建这个对象， 或者在 Pod 规约中将 ConfigMap 标记为 optional（请参阅可选的 ConfigMaps）。 如果所引用的 ConfigMap 不存在，并且没有将应用标记为 optional 则 Pod 将无法启动。 同样，引用 ConfigMap 中不存在的主键也会令 Pod 无法启动，除非你将 Configmap 标记为 optional。 如果你使用 envFrom 来基于 ConfigMap 定义环境变量，那么无效的键将被忽略。 Pod 可以被启动，但无效名称将被记录在事件日志中（InvalidVariableNames）。 日志消息列出了每个被跳过的键。例如:   可选的 ConfigMap 你可以在 Pod 规约中将对 ConfigMap 的引用标记为 可选（optional）。 如果 ConfigMap 不存在，那么它在 Pod 中为其提供数据的配置（例如环境变量、挂载的卷）将为空。 如果 ConfigMap 存在，但引用的键不存在，那么数据也是空的。"},{"id":10,"href":"/books/kustz/chapter03/04-container-resources/","title":"04 Container Resources","parent":"Chapter03","content":"3.4. 用字符串定义容器申请资源上下限      大家好， 我是老麦， 一个运维小学生。 今天我们来给 kustz 添加资源申请字段。\n  Pod 的资源申请， 在调度策略中， 是一个重要的参数数据。 因此其重要性自然不必多说\n容器资源申请     在官网中， 对于资源的申请和管理有详细的描述。 https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/\n和 服务质量 QoS 息息相关， https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/\n这里简单的归类， 可以速记， 按照服务质量高到低\n Guaranteed: request = limit Burstable: request \u0026lt; limit BestEffort: 没有 request 和 limit  kustz.yml 配置     还是先来看看 kustz.yml 配置中， 资源的抽象 name: request/limit。\n# kustz.ymlservice:resources:cpu:10m/20mmemory:10Mi/20Mi# nvidia.com/gpu: 1/1对应的， 在 /pkg/kustz/kustz.go 中， 增加字段， 如下。\ntype Service struct { Resources map[string]string `json:\u0026#34;resources,omitempty\u0026#34;` } 编码     这部分的编码还是还是很简单的。 因为在 k8s.io/apimachinery/pkg/api/resource 库中， 已经为我们提供了 数据及单位 的解析封装， 直接调用即可。\n所以大部分编码工作还是在字符串的解析上面。\n第一步， 在 /pkg/kustz/k_container.go 中， 为容器添加资源字段\nfunc (kz *Config) kubeContainerResources() corev1.ResourceRequirements { return tokube.ContainerResources(kz.Service.Resources) } 第二步， 在 /pkg/tokube/container_resources.go 中， 解析 map[string]string 对象， 获取资源名和值， 并返回 corev1.Resources\nfunc ContainerResources(res map[string]string) corev1.ResourceRequirements { // ...省略 } 第三步， 在 /pkg/tokube/container_resources.go 中分割我们定义的字符串。\nfunc toResourceQuantity(value string) (request resource.Quantity, limit resource.Quantity) { re, li := \u0026#34;\u0026#34;, \u0026#34;\u0026#34; parts := strings.Split(value, \u0026#34;/\u0026#34;) // ... 省略 \trequest = resource.MustParse(re) limit = resource.MustParse(li) return } 可以看到， 这里使用了官方的函数 resource.MustParse(value) ， 直接返回结论， 省了很多事情。\n测试     执行命令， 查看结果。\n$ make test.deployment 除了默认的 cpu, memory 之外， 我们还添加了 nvidia/gpu 这种 CRD 资源。\nGPU 结论可以参考， https://help.aliyun.com/document_detail/94800.html\n","description":"3.4. 用字符串定义容器申请资源上下限      大家好， 我是老麦， 一个运维小学生。 今天我们来给 kustz 添加资源申请字段。\n  Pod 的资源申请， 在调度策略中， 是一个重要的参数数据。 因此其重要性自然不必多说\n容器资源申请     在官网中， 对于资源的申请和管理有详细的描述。 https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/\n和 服务质量 QoS 息息相关， https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/\n这里简单的归类， 可以速记， 按照服务质量高到低\n Guaranteed: request = limit Burstable: request \u0026lt; limit BestEffort: 没有 request 和 limit  kustz.yml 配置     还是先来看看 kustz.yml 配置中， 资源的抽象 name: request/limit。\n# kustz.ymlservice:resources:cpu:10m/20mmemory:10Mi/20Mi# nvidia.com/gpu: 1/1对应的， 在 /pkg/kustz/kustz.go 中， 增加字段， 如下。"},{"id":11,"href":"/books/kustz/chapter03/05-container-probe/","title":"05 Container Probe","parent":"Chapter03","content":"3.5. 为 Container 添加健康检查方法      大家好， 我是老麦。 kustz 终于到了准生产的地步了。 今天的健康检查接口， 就为我们解决这个问题。\n  我们要知道， 确定一个应用能不能对外提供服务之前， 需要进行一个 可用性 检测。 而这个检测通常被我们称为 健康检查。\nKubernetes 的健康检查     在 Kubernetes 中， 为我们提供了 主要 的 3类状态 的健康检查。\n startup: 等待探针。 如果执行成功， 则再执行 liveness, readienss。 如果执行失败， 则遵循 restartPolicy 规则。 liveness: 存活探针， 如果失败， 服务将被重新发布(redeploy)。 readiness: 就绪探针， 如果失败， 服务不会加入到 service backend endpoints 中对外提供服务。   https://kubebyexample.com/learning-paths/application-development-kubernetes/lesson-4-customize-deployments-application-2\n  此外， Kubernetes 支持 4种类型 的检查方式\n httpGet: 检查 GET 接口返回值。 tcp: tcp 端口是否打开。 exec: 命令执行是否成功。 grpc: grpc 端口是否打开。  kustz.yml 配置     一如既往， 我们需要抽象一个简单明了的方法定义健康检查方法。 剩下的就交给 kustz 处理。\n# kustz.ymlservice:probes:liveness:action:http://:8080/healthyheaders:token:\u0026#34;token123\u0026#34;initialDelaySeconds:30readiness:action:tcp://0.0.0.0:8080startup:action:cat /tmp/healthy从配置中可以看到， 通过 action 我们提供了其中三种检查方法 httpGet, tcp, exec。\n grpc 现在还属于 1.24 还 beta 状态。 对于 grpc 我也不熟， 所以就不讨论了。\n 编码     这节不难\n数据结构定义     对应的在 /pkg/kustz/kustz.go 中添加如下结构。\ntype Service struct { Probes ContainerProbes `json:\u0026#34;probes,omitempty\u0026#34;` } type ContainerProbe struct { ProbeHandler `json:\u0026#34;,inline\u0026#34;` InitialDelaySeconds int32 `json:\u0026#34;initialDelaySeconds,omitempty\u0026#34;` TimeoutSeconds int32 `json:\u0026#34;timeoutSeconds,omitempty\u0026#34;` PeriodSeconds int32 `json:\u0026#34;periodSeconds,omitempty\u0026#34;` SuccessThreshold int32 `json:\u0026#34;successThreshold,omitempty\u0026#34;` FailureThreshold int32 `json:\u0026#34;failureThreshold,omitempty\u0026#34;` TerminationGracePeriodSeconds *int64 `json:\u0026#34;terminationGracePeriodSeconds,omitempty\u0026#34;` } 其中 ContainerProbe 基本上就是从 corev1.Probe 中直接复制过来的， 但又对 ProbeHandler 进行了一些本地化处理。\ntype ProbeHandler struct { Action string `json:\u0026#34;action,omitempty\u0026#34;` Headers map[string]string `json:\u0026#34;headers,omitempty\u0026#34;` } 处理 Action 生成 Handler     对于 Action 的值， 我们需要进行类型处理， 并返回相应的 corev1.ProbeHandler。\n在 /pkg/tokube/container_probe.go 中, 定义函数 ProbeHandler 处理 action 和 headers 的值并返回结果。\n// ProbeHandler action // // http(s)://:8080/healthy // tcp://:8080 // cat /tmp/healthy func ProbeHandler(action string, headers map[string]string) corev1.ProbeHandler { if strings.HasPrefix(action, \u0026#34;tcp://\u0026#34;) { return toTCPProbeHandler(action) } if strings.HasPrefix(action, \u0026#34;http://\u0026#34;) || strings.HasSuffix(action, \u0026#34;https://\u0026#34;) { return toHTTPProbeHandler(action, headers) } return toExecProbeHandler(action) } 可以看到， 通过判断 action 的前缀字符串的值， 确认了健康检查的方法。 而只有 httpGet 方法接受了 headers 参数。\n在 toTCPProbeHandler() 和 toHTTPProbeHandler() 中， 使用 url.Parse() 方法， 很简单的提取了所有字段数据。\n在 toExecProbeHandler 中， 直接使用 strings.Split() 方法分割。\n为 Container 添加健康检查     在 /pkg/kustz/k_container.go 中， 为 ContainerProbe 为添加方法 kubeProbe 创建健康检查， 这时添加所有附加参数。\n// kubeProbe return Kube Probe without handler func (cp *ContainerProbe) kubeProbe() *corev1.Probe { handler := tokube.ProbeHandler(cp.Action, cp.Headers) return \u0026amp;corev1.Probe{ ProbeHandler: handler, InitialDelaySeconds: cp.InitialDelaySeconds, TimeoutSeconds: cp.TimeoutSeconds, PeriodSeconds: cp.PeriodSeconds, SuccessThreshold: cp.SuccessThreshold, FailureThreshold: cp.FailureThreshold, TerminationGracePeriodSeconds: cp.TerminationGracePeriodSeconds, } } 为 ContainerProbes 添加 kubeProbe 方法， 解决 ContainerProbe 的为 nil 的问题。\nfunc (cps ContainerProbes) kubeProbe(cp *ContainerProbe) *corev1.Probe { if cp == nil { return nil } return cp.kubeProbe() } 测试     执行命令， 检查结果\n$ make test.deployment ","description":"3.5. 为 Container 添加健康检查方法      大家好， 我是老麦。 kustz 终于到了准生产的地步了。 今天的健康检查接口， 就为我们解决这个问题。\n  我们要知道， 确定一个应用能不能对外提供服务之前， 需要进行一个 可用性 检测。 而这个检测通常被我们称为 健康检查。\nKubernetes 的健康检查     在 Kubernetes 中， 为我们提供了 主要 的 3类状态 的健康检查。\n startup: 等待探针。 如果执行成功， 则再执行 liveness, readienss。 如果执行失败， 则遵循 restartPolicy 规则。 liveness: 存活探针， 如果失败， 服务将被重新发布(redeploy)。 readiness: 就绪探针， 如果失败， 服务不会加入到 service backend endpoints 中对外提供服务。   https://kubebyexample.com/learning-paths/application-development-kubernetes/lesson-4-customize-deployments-application-2\n  此外， Kubernetes 支持 4种类型 的检查方式"},{"id":12,"href":"/books/kustz/chapter03/06-image-pull-policy/","title":"06 Image Pull Policy","parent":"Chapter03","content":"3.6. 镜像拉取鉴权和策略      大家好， 我是老麦。 今天我们解决镜像拉取鉴权和策略\n  镜像拉取鉴权     拉取私有镜像或私有仓库镜像的时候， 需要提供鉴权信息。\n在 Kubernets 中， 通过 Secret 管理账号这些账号信息。 Secret 类型分为两种，\n  kubernetes.io/dockerconfigjson: 如果有linux安装了 docker， 就是 ~/.docker/config.json 这个文件。\n  kubernetes.io/dockercfg: 不太熟。\n  在 /pkg/tokube/pod.go 中， 可以看到 ImagePullSecrets 的处理方法。 就是将字符串转为 kubernetes 的引用对象， 其它没什么好说的。\nfunc ImagePullSecrets(secrets []string) []corev1.LocalObjectReference { if len(secrets) == 0 { return nil } objs := []corev1.LocalObjectReference{} for _, s := range secrets { objs = append(objs, corev1.LocalObjectReference{ Name: s, }) } return objs } 镜像拉去策略     镜像拉去策略分为三种， Never, Always, IfNotPresent\n在 /pkg/tokube/container.go 中， 可以看到 ImagePullPolicy 的处理方法。\nfunc ImagePullPolicy(s string) corev1.PullPolicy { switch strings.ToLower(s) { case \u0026#34;always\u0026#34;: return corev1.PullAlways case \u0026#34;never\u0026#34;: return corev1.PullNever case \u0026#34;ifnotpresent\u0026#34;: return corev1.PullIfNotPresent } return \u0026#34;\u0026#34; }  在 kustz.yml 不再大小写敏感， 因为我们将值全部转为小写。 当不指定配置策略的时候， 使用默认策略。  使用     如果在 kustz.yml 配置中， 通过如下配置。\n假设配置文件名为 docker-config.json， 支持多个账号， 参考如下。\n// docker-config.json { \u0026quot;auths\u0026quot;: { \u0026quot;ghcr.io\u0026quot;: { \u0026quot;auth\u0026quot;: \u0026quot;Abcdefg==\u0026quot; }, \u0026quot;https://index.docker.io/v1/\u0026quot;: { \u0026quot;auth\u0026quot;: \u0026quot;Abcdefg=\u0026quot; } } } auth 值是 user:password 的 base64 编码。 如果不知道怎么弄 docker login 生成\n$ docker login -u myUser -p myPass 在 kustz.yml 中， 通过 docker-config.json 创建 Secret 并引用。\nservice:imagePullSecrets:- aliyun-reposecrets:files:- name:aliyun-repofiles:- .dockerconfigjson=docker-config.jsontype:kubernetes.io/dockerconfigjson","description":"3.6. 镜像拉取鉴权和策略      大家好， 我是老麦。 今天我们解决镜像拉取鉴权和策略\n  镜像拉取鉴权     拉取私有镜像或私有仓库镜像的时候， 需要提供鉴权信息。\n在 Kubernets 中， 通过 Secret 管理账号这些账号信息。 Secret 类型分为两种，\n  kubernetes.io/dockerconfigjson: 如果有linux安装了 docker， 就是 ~/.docker/config.json 这个文件。\n  kubernetes.io/dockercfg: 不太熟。\n  在 /pkg/tokube/pod.go 中， 可以看到 ImagePullSecrets 的处理方法。 就是将字符串转为 kubernetes 的引用对象， 其它没什么好说的。\nfunc ImagePullSecrets(secrets []string) []corev1.LocalObjectReference { if len(secrets) == 0 { return nil } objs := []corev1.LocalObjectReference{} for _, s := range secrets { objs = append(objs, corev1."},{"id":13,"href":"/books/kustz/chapter04/01-kustz-flags/","title":"01 Kustz Flags","parent":"Chapter04","content":"4.1. 使用 cobrautils 为命令添加更实用的命令参数      大家好， 我是老麦。\n 之前的章节， 我们陆陆续续给 kustz 库添加了很多丰富服务的配置\n但 kustz 命令， 还是处于一个很原始的命令状态。 接下来我们给 kustz 添加一些更丰富的参数 ， 使 kustz 用起来更顺手。\n在 CICD 的中， 一般情况下 变量，健康检查， 镜像策略 等很难发生变动。 而镜像名称 经常性 的在每次打包后发生变化。\n每次CI触发都去修改 kustz.yml 配置显然是不可能的。 因此， 我们需要绑定更丰富的参数来支持我们 CI 的运行。\ncobra flag     之前在 /cmd/kustz/cmd/render.go 中， 我们为命令添加了一个指定配置文件的参数。\nfunc init() { cmdRender.Flags().StringVarP(\u0026amp;config, \u0026#34;config\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;kustz.yml\u0026#34;, \u0026#34;kustz config\u0026#34;) } var config string 这种方法是 cobra 官方提供的基本模式。 在绑定的时候， 需要一行写一个， 并且不支持 指针参数 。\ncobrautils 库     接下来我们使用自己封装的 cobrautils 库。\n$ go get -u github.com/go-jarvis/cobrautils 详细描述参考 https://github.com/go-jarvis/cobrautils\nfunc init() { cobrautils.BindFlags(cmdRender, flags) } // KustzFlag 定义 flag type KustzFlag struct { Config string `flag:\u0026#34;config\u0026#34; usage:\u0026#34;kustz config\u0026#34; shorthand:\u0026#34;c\u0026#34;` Image string `flag:\u0026#34;image\u0026#34; usage:\u0026#34;image name\u0026#34;` Replicas *int `flag:\u0026#34;replicas\u0026#34; usage:\u0026#34;pod replicas number\u0026#34;` } // 初始化默认值 var flags = \u0026amp;KustzFlag{ Config: \u0026#34;kustz.yml\u0026#34;, } 可以看到， 使用 cobrautils 之后。\n 使用结构体组合了所有参数， 每个字段通过注释描述， 作用更清晰， 耦合度更高。 支持 指针参数， 解决了 零值 带来的负面影响。 一行命令解决了所有参数的绑定。   其实如果喜欢的话， 可以将 /pkg/kustz/kustz.go 中的完整 Config 做成参数。\n 在 /cmd/kustz/cmd/render.go 中， 将 Image, Replicas 两个参数注入到配置文件中即可。\nfunc render() { kz := kustz.NewKustzFromConfig(flags.Config) if flags.Image != \u0026#34;\u0026#34; { kz.Service.Image = flags.Image } if flags.Replicas != nil { kz.Service.Replicas = int32(*flags.Replicas) } kz.RenderAll() } ","description":"4.1. 使用 cobrautils 为命令添加更实用的命令参数      大家好， 我是老麦。\n 之前的章节， 我们陆陆续续给 kustz 库添加了很多丰富服务的配置\n但 kustz 命令， 还是处于一个很原始的命令状态。 接下来我们给 kustz 添加一些更丰富的参数 ， 使 kustz 用起来更顺手。\n在 CICD 的中， 一般情况下 变量，健康检查， 镜像策略 等很难发生变动。 而镜像名称 经常性 的在每次打包后发生变化。\n每次CI触发都去修改 kustz.yml 配置显然是不可能的。 因此， 我们需要绑定更丰富的参数来支持我们 CI 的运行。\ncobra flag     之前在 /cmd/kustz/cmd/render.go 中， 我们为命令添加了一个指定配置文件的参数。\nfunc init() { cmdRender.Flags().StringVarP(\u0026amp;config, \u0026#34;config\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;kustz.yml\u0026#34;, \u0026#34;kustz config\u0026#34;) } var config string 这种方法是 cobra 官方提供的基本模式。 在绑定的时候， 需要一行写一个， 并且不支持 指针参数 。"},{"id":14,"href":"/books/kustz/chapter01/","title":"Chapter01","parent":"","content":"","description":""},{"id":15,"href":"/books/kustz/chapter02/","title":"Chapter02","parent":"","content":"","description":""},{"id":16,"href":"/books/kustz/chapter03/","title":"Chapter03","parent":"","content":"","description":""},{"id":17,"href":"/books/kustz/chapter04/","title":"Chapter04","parent":"","content":"","description":""},{"id":18,"href":"/books/kustz/tags/","title":"Tags","parent":"","content":"","description":""}]